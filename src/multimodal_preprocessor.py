import sys
import os

# è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)

# æ·»åŠ è·¯å¾„
sys.path.append(parent_dir)  # é¡¹ç›®æ ¹ç›®å½•
sys.path.append(current_dir)  # srcç›®å½•
import json
import fitz  # PyMuPDF
import torch
import numpy as np
from PIL import Image, ImageEnhance
from transformers import CLIPProcessor, CLIPModel
import datetime
import pandas as pd
import cv2
import easyocr
import re
import pdfplumber
import camelot
import csv
try:
    # æ‡’åŠ è½½é«˜çº§PPTXå¤„ç†å™¨ï¼ˆè‹¥ä¸å¯ç”¨åˆ™å¿½ç•¥ï¼‰
    from advanced_pptx_processor import process_pptx_file_advanced
except Exception:
    process_pptx_file_advanced = None

class MultimodalPreprocessor:
    def __init__(self):
        """åˆå§‹åŒ–å¤šæ¨¡æ€é¢„å¤„ç†å·¥å…·"""
        print("ğŸš€ å¼€å§‹åˆå§‹åŒ–å¤šæ¨¡æ€é¢„å¤„ç†å·¥å…·...")
        
        # æ£€æµ‹è®¾å¤‡
        print("ğŸ“± æ£€æµ‹è®¡ç®—è®¾å¤‡...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        print("ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•...")
        os.makedirs("output/text", exist_ok=True)
        os.makedirs("output/images", exist_ok=True)
        os.makedirs("output/formulas", exist_ok=True)
        os.makedirs("output/tables", exist_ok=True)
        os.makedirs("output/code", exist_ok=True)
        print("âœ“ è¾“å‡ºç›®å½•åˆ›å»ºå®Œæˆ")
        
        # åˆå§‹åŒ–CLIPæ¨¡å‹ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰
        print("ğŸ¤– æ­£åœ¨åŠ è½½CLIPæ¨¡å‹...")
        print("   â³ æœ¬åœ°æ¨¡å‹åŠ è½½ä¸­ï¼Œè¯·ç¨å€™...")
        try:
            self.clip_model = CLIPModel.from_pretrained("./clip-model").to(self.device)
            print("   âœ“ CLIPæ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"   âŒ æœ¬åœ°CLIPæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("   â³ å°è¯•åœ¨çº¿ä¸‹è½½CLIPæ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
            try:
                self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(self.device)
                print("   âœ“ åœ¨çº¿CLIPæ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ")
            except Exception as e2:
                print(f"   âŒ CLIPæ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥: {e2}")
                raise e2
        
        print("ğŸ”§ æ­£åœ¨åŠ è½½CLIPå¤„ç†å™¨...")
        print("   â³ å¤„ç†å™¨åŠ è½½ä¸­ï¼ˆå¯èƒ½éœ€è¦ä¸‹è½½ï¼‰...")
        try:
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            print("   âœ“ CLIPå¤„ç†å™¨åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"   âŒ CLIPå¤„ç†å™¨åŠ è½½å¤±è´¥: {e}")
            raise e
        
        # åˆå§‹åŒ–OCRå¼•æ“ï¼ˆé¦–æ¬¡è¿è¡Œè¾ƒæ…¢ï¼‰
        print("ğŸ‘ æ­£åœ¨åˆå§‹åŒ–OCRå¼•æ“...")
        print("   â³ é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
        print("   ğŸ“¥ æ­£åœ¨ä¸‹è½½ä¸­æ–‡å’Œè‹±æ–‡OCRæ¨¡å‹...")
        try:
            self.ocr_reader = easyocr.Reader(['ch_sim', 'en'], gpu=False)  # å¼ºåˆ¶ä½¿ç”¨CPUé¿å…GPUé—®é¢˜
            print("   âœ“ OCRå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"   âš  OCRåˆå§‹åŒ–å¤±è´¥ï¼Œå°†è·³è¿‡OCRåŠŸèƒ½: {e}")
            print("   å°†ç»§ç»­è¿è¡Œï¼Œä½†è·³è¿‡OCRå…¬å¼è¯†åˆ«åŠŸèƒ½")
            self.ocr_reader = None
        
        # å­˜å‚¨å¤„ç†ç»“æœ
        self.results = []
        
        print("ğŸ‰ å¤šæ¨¡æ€é¢„å¤„ç†å·¥å…·åˆå§‹åŒ–å®Œæˆï¼")
        print("=" * 50)

    def process_pdf(self, file_path):
        """å¤„ç†PDFæ–‡ä»¶ï¼Œæå–æ–‡æœ¬å’Œå›¾åƒ"""
        print(f"å¼€å§‹å¤„ç†PDFæ–‡ä»¶: {file_path}")
        doc = fitz.open(file_path)
        base_filename = os.path.splitext(os.path.basename(file_path))[0]
        
        # ä¸ºå½“å‰PDFæ–‡ä»¶å•ç‹¬è®°å½•ç»“æœ
        current_results = []
        original_results = self.results
        self.results = current_results
        
        try:
            for page_num in range(len(doc)):
                print(f"å¤„ç†ç¬¬ {page_num + 1}/{len(doc)} é¡µ...")
                page = doc.load_page(page_num)
                page_text = page.get_text()
                
                # å¤„ç†é¡µé¢å›¾åƒ
                image_list = page.get_images(full=True)
                page_images = []
                
                for img_index, img in enumerate(image_list):
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    
                    # ä¿å­˜åŸå§‹å›¾åƒ
                    img_path = f"output/images/{base_filename}_p{page_num+1}_img{img_index+1}.{base_image['ext']}"
                    with open(img_path, "wb") as img_file:
                        img_file.write(image_bytes)
                    
                    # å¤„ç†å›¾åƒå¹¶ä¿å­˜
                    image_data = self.process_image(img_path, page_text)
                    self.save_image_data(image_data, base_filename, page_num, img_index)
                    page_images.append(img_path)
                
                # å¤„ç†é¡µé¢æ–‡æœ¬
                text_data = self.process_text(page_text, page_num, page_images)
                text_data["source"] = f"{base_filename}_page{page_num+1}"
                self.save_text_data(text_data, base_filename, page_num)
                
                # æå–é¡µé¢ä¸­çš„å…¬å¼ã€è¡¨æ ¼ã€ä»£ç 
                self.extract_formulas_from_page(page, page_text, base_filename, page_num)
                self.extract_tables_from_page(page, page_text, base_filename, page_num)
                self.extract_code_from_page(page_text, base_filename, page_num)
            
            # ä¿å­˜PDFä¸“ç”¨å…ƒæ•°æ®
            self.save_pdf_metadata(file_path, base_filename)
            print(f"PDFå¤„ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨output/{base_filename}_pdf_metadata.json")
            
        finally:
            # æ¢å¤åŸå§‹ç»“æœåˆ—è¡¨å¹¶åˆå¹¶å½“å‰ç»“æœ
            self.results = original_results
            self.results.extend(current_results)

    def process_text(self, text, page_num, page_images):
        """å¤„ç†æ–‡æœ¬å†…å®¹"""
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = text.strip()
        if not cleaned_text:
            cleaned_text = "[é¡µé¢æ— æ–‡æœ¬å†…å®¹]"
        
        # ä½¿ç”¨CLIPç”Ÿæˆæ–‡æœ¬è¯­ä¹‰å‘é‡
        try:
            inputs = self.clip_processor(text=cleaned_text, return_tensors="pt", padding=True, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                text_features = self.clip_model.get_text_features(**inputs)
                text_vector = text_features.cpu().numpy()[0]
        except Exception as e:
            print(f"æ–‡æœ¬å‘é‡åŒ–å¤±è´¥: {e}")
            text_vector = np.zeros(512)  # CLIPé»˜è®¤å‘é‡ç»´åº¦
        
        return {
            "type": "text",
            "page": page_num + 1,
            "raw_text": cleaned_text,
            "word_count": len(cleaned_text),
            "associated_images": page_images,
            "text_vector": text_vector.tolist()
        }

    def process_image(self, image_path, page_text):
        """å¤„ç†å›¾åƒï¼ˆä½¿ç”¨CLIPï¼‰"""
        # å›¾åƒå¢å¼º
        enhanced_path = self.enhance_image(image_path)
        
        # è·å–å›¾åƒåŸºæœ¬ä¿¡æ¯
        try:
            with Image.open(image_path) as img:
                width, height = img.size
                format_type = img.format
                mode = img.mode
        except Exception as e:
            print(f"è¯»å–å›¾åƒä¿¡æ¯å¤±è´¥: {e}")
            width = height = 0
            format_type = mode = "unknown"
        
        # ä½¿ç”¨CLIPç”Ÿæˆå›¾åƒå‘é‡å’Œæè¿°
        try:
            image = Image.open(enhanced_path)
            inputs = self.clip_processor(images=image, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                image_features = self.clip_model.get_image_features(**inputs)
                image_vector = image_features.cpu().numpy()[0]
            
            # ç”Ÿæˆå›¾åƒæè¿°æ ‡ç­¾
            description_tags = self.generate_image_descriptions(enhanced_path)
            
        except Exception as e:
            print(f"å›¾åƒå¤„ç†å¤±è´¥: {e}")
            image_vector = np.zeros(512)
            description_tags = []
        
        return {
            "type": "image",
            "image_path": image_path,
            "enhanced_path": enhanced_path,
            "width": width,
            "height": height,
            "format": format_type,
            "mode": mode,
            "page_text_context": page_text[:200] + "..." if len(page_text) > 200 else page_text,
            "image_vector": image_vector.tolist(),
            "clip_descriptions": description_tags
        }

    def enhance_image(self, image_path):
        """å›¾åƒå¢å¼ºå¤„ç†"""
        img = Image.open(image_path)
        
        # å¯¹æ¯”åº¦å¢å¼º
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(1.5)
        
        # é”åº¦å¢å¼º
        enhancer = ImageEnhance.Sharpness(img)
        img = enhancer.enhance(2.0)
        
        # ä¿å­˜å¢å¼ºåçš„å›¾åƒ
        enhanced_path = image_path.replace(".", "_enhanced.")
        img.save(enhanced_path)
        
        return enhanced_path

    def clip_generate_description(self, image_path: str) -> str:
        """åŸºäºCLIPä¸ºå›¾ç‰‡ç”Ÿæˆæè¿°æ–‡æœ¬å¹¶ä¿å­˜ä¸ºJSONï¼Œè¿”å›æè¿°æ–‡ä»¶è·¯å¾„ã€‚"""
        try:
            descriptions = self.generate_image_descriptions(image_path)
        except Exception as e:
            print(f"ç”Ÿæˆå›¾ç‰‡æè¿°å¤±è´¥: {e}")
            descriptions = []

        base, _ = os.path.splitext(os.path.basename(image_path))
        desc_path = os.path.join("output", "images", f"{base}_desc.json")
        try:
            with open(desc_path, "w", encoding="utf-8") as f:
                json.dump({
                    "image_path": image_path,
                    "clip_descriptions": descriptions
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜å›¾ç‰‡æè¿°å¤±è´¥: {e}")
        return desc_path

    def generate_image_descriptions(self, image_path):
        """ä½¿ç”¨CLIPç”Ÿæˆå›¾åƒæè¿°æ ‡ç­¾"""
        try:
            image = Image.open(image_path)
            image_input = self.clip_processor(images=image, return_tensors="pt").to(self.device)
            
            # é¢„å®šä¹‰çš„æè¿°å€™é€‰åˆ—è¡¨
            text_descriptions = [
                "ç§‘å­¦å›¾è¡¨", "æ•°å­¦å…¬å¼", "æ•°æ®å›¾è¡¨", "æµç¨‹å›¾", 
                "å®éªŒè£…ç½®", "åˆ†å­ç»“æ„", "å‡ ä½•å›¾å½¢", "ç»Ÿè®¡å›¾è¡¨",
                "æŠ€æœ¯ç¤ºæ„å›¾", "æ¦‚å¿µå›¾", "ç½‘ç»œå›¾", "ç³»ç»Ÿæ¶æ„å›¾",
                "ç…§ç‰‡", "æ’å›¾", "ç¤ºä¾‹å›¾", "å¯¹æ¯”å›¾",
                "æ–‡æœ¬å›¾åƒ", "è¡¨æ ¼", "ä»£ç ", "æˆªå›¾"
            ]
            
            text_inputs = self.clip_processor(
                text=text_descriptions, 
                return_tensors="pt", 
                padding=True, 
                truncation=True
            ).to(self.device)
            
            with torch.no_grad():
                image_features = self.clip_model.get_image_features(**image_input)
                text_features = self.clip_model.get_text_features(**text_inputs)
                
                # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = (image_features @ text_features.T).softmax(dim=-1)
                values, indices = similarity[0].topk(5)  # å–å‰5ä¸ªæœ€ç›¸ä¼¼çš„æè¿°
                
                descriptions = []
                for value, idx in zip(values.cpu().numpy(), indices.cpu().numpy()):
                    descriptions.append({
                        "description": text_descriptions[idx],
                        "confidence": float(value)
                    })
            
            return descriptions
            
        except Exception as e:
            print(f"å›¾åƒæè¿°ç”Ÿæˆé”™è¯¯: {image_path}, {str(e)}")
            return []

    def save_text_data(self, data, filename, page_num):
        """ä¿å­˜æ–‡æœ¬å¤„ç†ç»“æœ"""
        output_path = f"output/text/{filename}_p{page_num+1}.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.results.append({
            "type": "text",
            "page": page_num + 1,
            "file": output_path
        })

    def save_image_data(self, data, filename, page_num, img_index):
        """ä¿å­˜å›¾åƒå¤„ç†ç»“æœ"""
        output_path = f"output/images/{filename}_p{page_num+1}_img{img_index+1}.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.results.append({
            "type": "image",
            "page": page_num + 1,
            "file": output_path
        })

    def save_pdf_metadata(self, file_path, filename):
        """ä¿å­˜PDFä¸“ç”¨å…ƒæ•°æ®æ–‡ä»¶"""
        # ç»Ÿè®¡ä¿¡æ¯
        text_files = [r for r in self.results if r["type"] == "text"]
        image_files = [r for r in self.results if r["type"] == "image"]
        formula_files = [r for r in self.results if r["type"] == "formula"]
        table_files = [r for r in self.results if r["type"] == "table"]
        code_files = [r for r in self.results if r["type"] == "code"]
        
        # è®¡ç®—è¡¨æ ¼ç»Ÿè®¡
        total_table_rows = sum(r.get("rows", 0) for r in table_files)
        total_table_columns = sum(r.get("columns", 0) for r in table_files)
        
        # è®¡ç®—ä»£ç ç»Ÿè®¡
        total_code_lines = sum(r.get("line_count", 0) for r in code_files)
        code_languages = list(set(r.get("language", "unknown") for r in code_files))
        
        metadata = {
            "processing_date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source_file": filename,
            "source_path": file_path,
            "file_type": "PDF",
            "processing_method": "pymupdf_ocr_camelot",
            "statistics": {
                "total_pages": len(set(r["page"] for r in self.results)),
                "total_text_blocks": len(text_files),
                "total_images": len(image_files),
                "total_formulas": len(formula_files),
                "total_tables": len(table_files),
                "total_table_rows": total_table_rows,
                "total_table_columns": total_table_columns,
                "total_code_blocks": len(code_files),
                "total_code_lines": total_code_lines,
                "code_languages": code_languages
            },
            "files": {
                "text_files": text_files,
                "image_files": image_files,
                "formula_files": formula_files,
                "table_files": table_files,
                "code_files": code_files
            },
            "processing_info": {
                "clip_model": "openai/clip-vit-base-patch32",
                "device": self.device,
                "output_format": "JSON/CSV",
                "ocr_enabled": self.ocr_reader is not None,
                "extraction_features": ["text", "images", "formulas", "tables", "code"]
            }
        }
        
        with open(f"output/{filename}_pdf_metadata.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

    def extract_formulas_from_page(self, page, page_text, filename, page_num):
        """ä»é¡µé¢ä¸­æå–æ•°å­¦å…¬å¼"""
        formulas = []
        
        # 1. ä»æ–‡æœ¬ä¸­æå–LaTeXæ ¼å¼çš„å…¬å¼
        latex_patterns = [
            r'\$\$([^$]+)\$\$',  # å—çº§å…¬å¼ $$...$$
            r'\$([^$]+)\$',      # è¡Œå†…å…¬å¼ $...$
            r'\\begin\{equation\}(.*?)\\end\{equation\}',  # equationç¯å¢ƒ
            r'\\begin\{align\}(.*?)\\end\{align\}',        # alignç¯å¢ƒ
            r'\\begin\{math\}(.*?)\\end\{math\}',          # mathç¯å¢ƒ
        ]
        
        for i, pattern in enumerate(latex_patterns):
            matches = re.findall(pattern, page_text, re.DOTALL)
            for j, match in enumerate(matches):
                formula_data = {
                    "type": "formula",
                    "page": page_num + 1,
                    "formula_id": f"{filename}_p{page_num+1}_formula{len(formulas)+1}",
                    "content": match.strip(),
                    "format": "latex",
                    "extraction_method": f"pattern_{i+1}",
                    "context": self.get_text_context(page_text, match, 100)
                }
                formulas.append(formula_data)
        
        # 2. ä»å›¾åƒä¸­è¯†åˆ«å…¬å¼ï¼ˆä½¿ç”¨OCRï¼‰- ç®€åŒ–ç‰ˆæœ¬é¿å…å¡ä½
        if self.ocr_reader and len(formulas) < 5:  # é™åˆ¶OCRå¤„ç†ï¼Œé¿å…å¡ä½
            try:
                # è·å–é¡µé¢å›¾åƒ
                pix = page.get_pixmap()
                img_data = pix.tobytes("png")
                img_array = np.frombuffer(img_data, np.uint8)
                img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                
                # OCRè¯†åˆ«ï¼Œè®¾ç½®è¶…æ—¶
                results = self.ocr_reader.readtext(img, width_ths=0.7, height_ths=0.7)
                
                for result in results[:3]:  # åªå¤„ç†å‰3ä¸ªç»“æœï¼Œé¿å…è¿‡å¤šå¤„ç†
                    text = result[1]
                    confidence = result[2]
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­¦ç¬¦å·
                    math_symbols = ['âˆ‘', 'âˆ«', 'âˆ‚', 'âˆ†', 'âˆ‡', 'âˆ', 'Â±', 'â‰ ', 'â‰¤', 'â‰¥', 'Î±', 'Î²', 'Î³', 'Î´', 'Î¸', 'Î»', 'Î¼', 'Ï€', 'Ïƒ', 'Ï†', 'Ïˆ', 'Ï‰']
                    if any(symbol in text for symbol in math_symbols) and confidence > 0.6:
                        if any(char.isdigit() or char in '+-*/=()[]{}^_' for char in text):
                            formula_data = {
                                "type": "formula",
                                "page": page_num + 1,
                                "formula_id": f"{filename}_p{page_num+1}_formula{len(formulas)+1}",
                                "content": text,
                                "format": "ocr_text",
                                "confidence": float(confidence),
                                "extraction_method": "ocr",
                                "bbox": [[float(pt[0]), float(pt[1])] for pt in result[0]]  # è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
                            }
                            formulas.append(formula_data)
            
            except Exception as e:
                print(f"OCRå…¬å¼è¯†åˆ«å¤±è´¥ (é¡µé¢ {page_num+1}): {e}")
        
        # ä¿å­˜å…¬å¼æ•°æ®
        if formulas:
            self.save_formulas_data(formulas, filename, page_num)
        
        return formulas

    def extract_tables_from_page(self, page, page_text, filename, page_num):
        """ä»é¡µé¢ä¸­æå–è¡¨æ ¼"""
        tables = []
        
        try:
            # ä½¿ç”¨camelotæå–è¡¨æ ¼ - é™åˆ¶å¤„ç†æ—¶é—´ï¼Œé¿å…å¡ä½
            pdf_path = None
            for file in os.listdir("input"):
                if file.lower().endswith('.pdf') and filename in file:
                    pdf_path = os.path.join("input", file)
                    break
            
            if pdf_path and os.path.exists(pdf_path) and page_num < 10:  # åªå¤„ç†å‰10é¡µï¼Œé¿å…å¡ä½
                # æå–å½“å‰é¡µé¢çš„è¡¨æ ¼ï¼Œé™åˆ¶å¤„ç†
                camelot_tables = camelot.read_pdf(pdf_path, pages=str(page_num+1), flavor='lattice')
                
                for i, table in enumerate(camelot_tables[:2]):  # åªå¤„ç†å‰2ä¸ªè¡¨æ ¼
                    if table.df is not None and not table.df.empty and len(table.df) > 0:
                        table_data = {
                            "type": "table",
                            "page": page_num + 1,
                            "table_id": f"{filename}_p{page_num+1}_table{i+1}",
                            "rows": len(table.df),
                            "columns": len(table.df.columns),
                            "data": table.df.to_dict('records'),
                            "extraction_method": "camelot",
                            "accuracy": getattr(table, 'accuracy', 0.0)
                        }
                        tables.append(table_data)
        
        except Exception as e:
            print(f"Camelotè¡¨æ ¼æå–è·³è¿‡ (é¡µé¢ {page_num+1}): {e}")
        
        # å¤‡ç”¨æ–¹æ³•ï¼šä»æ–‡æœ¬ä¸­è¯†åˆ«è¡¨æ ¼æ¨¡å¼
        table_patterns = self.detect_text_tables(page_text)
        for i, pattern in enumerate(table_patterns):
            table_data = {
                "type": "table",
                "page": page_num + 1,
                "table_id": f"{filename}_p{page_num+1}_texttable{i+1}",
                "content": pattern,
                "extraction_method": "text_pattern",
                "context": self.get_text_context(page_text, pattern, 50)
            }
            tables.append(table_data)
        
        # ä¿å­˜è¡¨æ ¼æ•°æ®
        if tables:
            self.save_tables_data(tables, filename, page_num)
        
        return tables

    def extract_code_from_page(self, page_text, filename, page_num):
        """ä»é¡µé¢æ–‡æœ¬ä¸­æå–ä»£ç å—"""
        code_blocks = []
        
        # ä»£ç å—æ¨¡å¼
        code_patterns = [
            r'```(\w*)\n(.*?)```',  # Markdownä»£ç å—
            r'`([^`]+)`',           # è¡Œå†…ä»£ç 
            r'(?:^|\n)((?:    |\t)[^\n]+(?:\n(?:    |\t)[^\n]+)*)',  # ç¼©è¿›ä»£ç å—
        ]
        
        # ç¼–ç¨‹è¯­è¨€å…³é”®å­—
        programming_keywords = [
            'def ', 'class ', 'import ', 'from ', 'if ', 'else:', 'for ', 'while ', 'return',
            'function', 'var ', 'let ', 'const ', 'console.log', 'print(', 'println',
            'public ', 'private ', 'static ', 'void ', 'int ', 'String ', 'boolean',
            '#include', 'using namespace', 'int main(', 'printf(', 'cout <<'
        ]
        
        for i, pattern in enumerate(code_patterns):
            matches = re.findall(pattern, page_text, re.DOTALL | re.MULTILINE)
            
            for j, match in enumerate(matches):
                if isinstance(match, tuple):
                    language = match[0] if match[0] else "unknown"
                    content = match[1] if len(match) > 1 else match[0]
                else:
                    content = match
                    language = "unknown"
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¼–ç¨‹å…³é”®å­—
                if any(keyword in content for keyword in programming_keywords) or len(content.strip()) > 20:
                    code_data = {
                        "type": "code",
                        "page": page_num + 1,
                        "code_id": f"{filename}_p{page_num+1}_code{len(code_blocks)+1}",
                        "content": content.strip(),
                        "language": language,
                        "extraction_method": f"pattern_{i+1}",
                        "line_count": len(content.strip().split('\n')),
                        "context": self.get_text_context(page_text, content, 100)
                    }
                    code_blocks.append(code_data)
        
        # ä¿å­˜ä»£ç æ•°æ®
        if code_blocks:
            self.save_code_data(code_blocks, filename, page_num)
        
        return code_blocks

    def get_text_context(self, full_text, target_text, context_length=100):
        """è·å–ç›®æ ‡æ–‡æœ¬çš„ä¸Šä¸‹æ–‡"""
        try:
            index = full_text.find(target_text)
            if index == -1:
                return target_text
            
            start = max(0, index - context_length)
            end = min(len(full_text), index + len(target_text) + context_length)
            return full_text[start:end]
        except:
            return target_text

    def detect_text_tables(self, text):
        """ä»æ–‡æœ¬ä¸­æ£€æµ‹è¡¨æ ¼æ¨¡å¼"""
        tables = []
        lines = text.split('\n')
        
        # å¯»æ‰¾åŒ…å«å¤šä¸ªåˆ¶è¡¨ç¬¦æˆ–ç©ºæ ¼åˆ†éš”çš„è¡Œ
        table_lines = []
        for line in lines:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼ç‰¹å¾ï¼šå¤šä¸ªåˆ¶è¡¨ç¬¦ã€ç«–çº¿åˆ†éš”ç¬¦ç­‰
            if '\t' in line and line.count('\t') >= 2:
                table_lines.append(line)
            elif '|' in line and line.count('|') >= 2:
                table_lines.append(line)
            elif re.search(r'\s{3,}', line) and len(line.split()) >= 3:
                table_lines.append(line)
            else:
                if table_lines and len(table_lines) >= 2:
                    tables.append('\n'.join(table_lines))
                table_lines = []
        
        # æ£€æŸ¥æœ€åä¸€ç»„
        if table_lines and len(table_lines) >= 2:
            tables.append('\n'.join(table_lines))
        
        return tables

    def save_formulas_data(self, formulas, filename, page_num):
        """ä¿å­˜å…¬å¼æ•°æ®"""
        # JSONæ ¼å¼ä¿å­˜
        json_path = f"output/formulas/{filename}_p{page_num+1}_formulas.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(formulas, f, ensure_ascii=False, indent=2)
        
        # CSVæ ¼å¼ä¿å­˜
        csv_path = f"output/formulas/{filename}_p{page_num+1}_formulas.csv"
        if formulas:
            df = pd.DataFrame(formulas)
            df.to_csv(csv_path, index=False, encoding="utf-8")
        
        # è®°å½•åˆ°ç»“æœ
        for formula in formulas:
            self.results.append({
                "type": "formula",
                "page": page_num + 1,
                "file": json_path,
                "formula_id": formula["formula_id"]
            })

    def save_tables_data(self, tables, filename, page_num):
        """ä¿å­˜è¡¨æ ¼æ•°æ®"""
        # JSONæ ¼å¼ä¿å­˜
        json_path = f"output/tables/{filename}_p{page_num+1}_tables.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(tables, f, ensure_ascii=False, indent=2)
        
        # ä¸ºæ¯ä¸ªè¡¨æ ¼å•ç‹¬ä¿å­˜CSV
        for i, table in enumerate(tables):
            if table.get("data") and isinstance(table["data"], list):
                csv_path = f"output/tables/{table['table_id']}.csv"
                try:
                    df = pd.DataFrame(table["data"])
                    df.to_csv(csv_path, index=False, encoding="utf-8")
                except Exception as e:
                    print(f"ä¿å­˜è¡¨æ ¼CSVå¤±è´¥: {e}")
        
        # è®°å½•åˆ°ç»“æœ
        for table in tables:
            self.results.append({
                "type": "table",
                "page": page_num + 1,
                "file": json_path,
                "table_id": table["table_id"],
                "rows": table.get("rows", 0),
                "columns": table.get("columns", 0)
            })

    def save_code_data(self, code_blocks, filename, page_num):
        """ä¿å­˜ä»£ç æ•°æ®"""
        # JSONæ ¼å¼ä¿å­˜
        json_path = f"output/code/{filename}_p{page_num+1}_code.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(code_blocks, f, ensure_ascii=False, indent=2)
        
        # CSVæ ¼å¼ä¿å­˜
        csv_path = f"output/code/{filename}_p{page_num+1}_code.csv"
        if code_blocks:
            df = pd.DataFrame(code_blocks)
            df.to_csv(csv_path, index=False, encoding="utf-8")
        
        # ä¸ºæ¯ä¸ªä»£ç å—å•ç‹¬ä¿å­˜æ–‡ä»¶
        for code in code_blocks:
            if code.get("language") and code.get("language") != "unknown":
                ext = self.get_file_extension(code["language"])
                code_file_path = f"output/code/{code['code_id']}.{ext}"
                with open(code_file_path, "w", encoding="utf-8") as f:
                    f.write(code["content"])
        
        # è®°å½•åˆ°ç»“æœ
        for code in code_blocks:
            self.results.append({
                "type": "code",
                "page": page_num + 1,
                "file": json_path,
                "code_id": code["code_id"],
                "language": code.get("language", "unknown"),
                "line_count": code.get("line_count", 0)
            })

    def get_file_extension(self, language):
        """æ ¹æ®ç¼–ç¨‹è¯­è¨€è·å–æ–‡ä»¶æ‰©å±•å"""
        extensions = {
            "python": "py",
            "javascript": "js",
            "java": "java",
            "cpp": "cpp",
            "c": "c",
            "csharp": "cs",
            "php": "php",
            "ruby": "rb",
            "go": "go",
            "rust": "rs",
            "swift": "swift",
            "kotlin": "kt",
            "typescript": "ts",
            "html": "html",
            "css": "css",
            "sql": "sql",
            "shell": "sh",
            "bash": "sh",
            "powershell": "ps1"
        }
        return extensions.get(language.lower(), "txt")

if __name__ == "__main__":
    print("=" * 60)
    print("å¤šæ¨¡æ€PDFæ•°æ®é¢„å¤„ç†å™¨")
    print("=" * 60)
    print("å¼€å§‹åˆå§‹åŒ–...")  # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs("output/text", exist_ok=True)
    os.makedirs("output/images", exist_ok=True)
    os.makedirs("output/formulas", exist_ok=True)
    os.makedirs("output/tables", exist_ok=True)
    os.makedirs("output/code", exist_ok=True)
    
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        print("âš™ï¸ æ­£åœ¨åˆå§‹åŒ–å¤„ç†å™¨...")
        print("âš ï¸ æ³¨æ„ï¼šé¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        processor = MultimodalPreprocessor()
        print("âœ… å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ!")
        
        # æ£€æŸ¥è¾“å…¥ç›®å½•ä¸­çš„PDFæ–‡ä»¶
        input_dir = "input"
        if not os.path.exists(input_dir):
            os.makedirs(input_dir, exist_ok=True)
            print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»º: {input_dir}")
            print("è¯·å°†PDFæ–‡ä»¶æ”¾å…¥inputç›®å½•åé‡æ–°è¿è¡Œ")
        else:
            # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ï¼Œè¿‡æ»¤æ‰ä¸´æ—¶æ–‡ä»¶
            input_files = [f for f in os.listdir(input_dir) 
                          if f.lower().endswith(('.pdf', '.pptx')) and not f.startswith('~$')]

            if not input_files:
                print(f"âŒ åœ¨ {input_dir} ç›®å½•ä¸­æœªæ‰¾åˆ°PDF/PPTXæ–‡ä»¶")
                print("è¯·å°†PDFæˆ–PPTXæ–‡ä»¶æ”¾å…¥inputç›®å½•åé‡æ–°è¿è¡Œ")
            else:
                pdf_count = sum(1 for f in input_files if f.lower().endswith('.pdf'))
                pptx_count = sum(1 for f in input_files if f.lower().endswith('.pptx'))
                print(f"âœ“ æ‰¾åˆ° {pdf_count} ä¸ªPDFæ–‡ä»¶, {pptx_count} ä¸ªPPTXæ–‡ä»¶")

                for idx, in_file in enumerate(input_files, 1):
                    input_path = os.path.join(input_dir, in_file)
                    print(f"\nğŸ“„ [{idx}/{len(input_files)}] å¼€å§‹å¤„ç†: {in_file}")
                    
                    if in_file.lower().endswith('.pdf'):
                        print("   ğŸ“š ä½¿ç”¨PDFå¤„ç†å™¨ï¼ˆPyMuPDF + OCR + è¡¨æ ¼æå–ï¼‰...")
                        processor.process_pdf(input_path)
                    elif in_file.lower().endswith('.pptx'):
                        if process_pptx_file_advanced is None:
                            print("   âš  æœªå®‰è£…PPTXå¤„ç†ä¾èµ–æˆ–å¯¼å…¥å¤±è´¥ï¼Œè·³è¿‡PPTXæ–‡ä»¶")
                        else:
                            print("   ğŸ“Š ä½¿ç”¨é«˜çº§PPTXå¤„ç†å™¨ï¼ˆZIP+XMLè§£æ + ä¼˜åŒ–è¡¨æ ¼æå–ï¼‰...")
                            process_pptx_file_advanced(processor, input_path)
                    
                    print(f"   âœ… [{idx}/{len(input_files)}] å®Œæˆå¤„ç†: {in_file}")

                print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")
                print("ğŸ“ ç»“æœä¿å­˜åœ¨: output/ ç›®å½•")
                
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()